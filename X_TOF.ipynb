{"cells":[{"cell_type":"markdown","metadata":{"id":"yK-ONXiYqxMW"},"source":["## **ToF Imaging**\n","\n","A **time-of-flight camera (ToF camera)** is a range imaging camera system for measuring distances between the camera and the subject for each point of the image based on time-of-flight (the round trip time of an artificial light signal, as provided by a laser or an LED). Laser-based time-of-flight cameras are part of a broader class of scannerless LIDAR, in which the entire scene is captured with each laser pulse, as opposed to point-by-point with a laser beam such as in scanning LIDAR systems."]},{"cell_type":"code","source":["!cat /proc/cpuinfo"],"metadata":{"id":"8cAtVwV5G6jr","executionInfo":{"status":"ok","timestamp":1697621204284,"user_tz":-120,"elapsed":5,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}},"outputId":"c87589f4-5eb3-4583-95fe-9837057fed72","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["processor\t: 0\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 79\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 0\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2199.998\n","cache size\t: 56320 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 0\n","initial apicid\t: 0\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n","bogomips\t: 4399.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n","processor\t: 1\n","vendor_id\t: GenuineIntel\n","cpu family\t: 6\n","model\t\t: 79\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","stepping\t: 0\n","microcode\t: 0xffffffff\n","cpu MHz\t\t: 2199.998\n","cache size\t: 56320 KB\n","physical id\t: 0\n","siblings\t: 2\n","core id\t\t: 0\n","cpu cores\t: 1\n","apicid\t\t: 1\n","initial apicid\t: 1\n","fpu\t\t: yes\n","fpu_exception\t: yes\n","cpuid level\t: 13\n","wp\t\t: yes\n","flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n","bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa mmio_stale_data retbleed\n","bogomips\t: 4399.99\n","clflush size\t: 64\n","cache_alignment\t: 64\n","address sizes\t: 46 bits physical, 48 bits virtual\n","power management:\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"jwkgHOsnqxMX"},"source":["The task involves **classifying the images into two classes**: background and robot."]},{"cell_type":"markdown","metadata":{"id":"yyNJMJxoqxMX"},"source":["The dataset [Miniature mobile robot detection using an utra-low resolution time-of-flight sensor](https://ieee-dataport.org/documents/miniature-mobile-robot-detection-using-ultra-low-resolution-time-flight-sensor-dataset) is composed of ToF depth images acquired with the [ST VL53L5CX ToF sensor](https://www.st.com/en/imaging-and-photonics-solutions/vl53l5cx.html) and sampled by a miniature mobile robot navigating in a sand-like terrain. Each image has a resolution of 8x8 pixels. It comprises a total of 4.150 samples, with 2.062 samples belonging to the\n","background class and 2.088 samples to the Robot class. The bit depth is 8 bits per pixel, where each pixel value represents the depth information."]},{"cell_type":"markdown","metadata":{"id":"FZvDBg5SqxMY"},"source":["# **Download dataset**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Mh98JQrIqxMY","executionInfo":{"status":"ok","timestamp":1699436180784,"user_tz":-60,"elapsed":2835,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["import urllib.request\n","\n","url = \"https://www.dropbox.com/s/4txj0ob6ovy9jbr/time-of-flight.zip?dl=1\"\n","u = urllib.request.urlopen(url)\n","data = u.read()\n","u.close()\n","with open(\"./time-of-flight.zip\", \"wb\") as f :\n","   f.write(data)"]},{"cell_type":"markdown","metadata":{"id":"kdfiN3mRqxMY"},"source":["Extract files:"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vGGuY8EsqxMZ","executionInfo":{"status":"ok","timestamp":1699436182267,"user_tz":-60,"elapsed":1488,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["import zipfile\n","\n","with zipfile.ZipFile(\"time-of-flight.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(\".\")"]},{"cell_type":"markdown","metadata":{"id":"C9o1ic6iqxMZ"},"source":["Just for Mac users, remove the .DS_Store files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FooNHspqxMZ"},"outputs":[],"source":["import os\n","\n","for root, dirs, files in os.walk('./time-of-flight/'):\n","    for file in files:\n","        if file.endswith('.DS_Store'):\n","            path = os.path.join(root, file)\n","            print(\"Deleting: %s\" % (path))\n","            if os.remove(path):\n","                print(\"Unable to delete!\")\n","            else:\n","                print(\"Deleted...\")"]},{"cell_type":"markdown","metadata":{"id":"d2_s-MDlqxMa"},"source":["Import the dataset from files:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IoTMnJWDqxMa","executionInfo":{"status":"ok","timestamp":1699436184622,"user_tz":-60,"elapsed":2358,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["import sklearn.datasets\n","\n","dataset = sklearn.datasets.load_files('./time-of-flight', shuffle='True', encoding='utf-8')"]},{"cell_type":"markdown","metadata":{"id":"8j8ppLxnqxMa"},"source":["Check the number of samples (should be 4150):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqQ6aKsSqxMa"},"outputs":[],"source":["samples_number = len(dataset.data)\n","print(\"Number of samples: \", samples_number)"]},{"cell_type":"markdown","metadata":{"id":"knIUQ0G5qxMa"},"source":["Pre-process files content in order to extract just image values from the CSV structure:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"B4werPLuqxMb","executionInfo":{"status":"ok","timestamp":1699436185025,"user_tz":-60,"elapsed":407,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["for index, data in enumerate(dataset.data) :\n","    data = data.split('\\n')\n","    data = data[1].split(',')\n","    dataset.data[index] = [int(i) for i in data]"]},{"cell_type":"markdown","metadata":{"id":"jyn-YRKQqxMb"},"source":["Show some sample as a grayscale image:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"UF3qKa8uqxMb","colab":{"base_uri":"https://localhost:8080/","height":693},"executionInfo":{"status":"ok","timestamp":1699436192128,"user_tz":-60,"elapsed":7105,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}},"outputId":"7e0a906a-944e-47ae-ca15-86c292da6de1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 800x800 with 20 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","w = 10\n","h = 10\n","fig = plt.figure(figsize=(8, 8))\n","columns = 4\n","rows = 5\n","\n","for i in range(1, columns*rows +1):\n","    index = random.randint(0, samples_number)\n","    img = np.array(dataset.data[index]).reshape(8,8)\n","    img = img.astype(float)\n","    img = img / np.max(img)\n","    fig.add_subplot(rows, columns, i)\n","    plt.imshow(img, cmap='gray')\n","    if(dataset.target[index] == 0):\n","        plt.title(\"Background\", fontsize=8)\n","    else:\n","        plt.title(\"Robot\", fontsize=8)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Zku4ylweqxMb"},"source":["Prepare the dataset for training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRa_Y7wBqxMb"},"outputs":[],"source":["X = np.array(dataset.data)/255\n","y = np.array(dataset.target)\n","print(\"Shape X: \", X.shape)\n","print(\"Shape y: \", y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sW8xyWBbqxMb"},"outputs":[],"source":["print(\"Samples number 1: \", np.count_nonzero(y == 1))\n","print(\"Samples number 0: \", np.count_nonzero(y == 0))"]},{"cell_type":"markdown","source":["Get the **height**, **width** and **number of channels** of the images.\n","Preprocessing the images by changing the size from 8 X 8 to 4X 4 or 2 X 2"],"metadata":{"id":"25pky3Vh7tIX"}},{"cell_type":"code","execution_count":10,"metadata":{"id":"jCLHFd-M3IzC","executionInfo":{"status":"ok","timestamp":1699436192129,"user_tz":-60,"elapsed":5,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["for index, data in enumerate(dataset.data) :\n","    dataset.data[index] = np.array(dataset.data[index]).reshape((8,8))\n","    ###### un comment the following line if for dataset reshaping into 4 by 4\n","    # dataset.data[index] = np.array(dataset.data[index]).reshape((4, 2, 4, 2)).mean(axis=(1, 3))\n","    ###### un comment the following line if for dataset reshaping into 2 by 2\n","    # dataset.data[index] = np.array(dataset.data[index]).reshape((2, 4, 2, 4)).mean(axis=(1, 3))\n","\n","height,width=dataset.data[0].shape\n","number_of_channels=1"]},{"cell_type":"markdown","metadata":{"id":"08rjB07ZqxMc"},"source":["##The two classes are balanced, so accuracy is a good metric to use\n","Preprocessing and Spliting the dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qUG_p07arS3S"},"outputs":[],"source":["X = np.array(dataset.data, dtype='float32')\n","y = np.array(dataset.target)\n","train_size=int(len(dataset.data) * 0.80)\n","X_train_full, X_test, y_train_full, y_test = X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n","val_size= int(len(dataset.data) * 0.15)\n","X_valid, X_train = X_train_full[:val_size], X_train_full[val_size:]\n","y_valid, y_train = y_train_full[:val_size], y_train_full[val_size:]\n","\n","print(\"Training set, Samples number 1: \", np.count_nonzero( y_train_full == 1))\n","print(\"Training set, Samples number 0: \", np.count_nonzero( y_train_full == 0))\n","\n","print(\"Test set, Samples number 1: \", np.count_nonzero( y_test == 1))\n","print(\"Test set, Samples number 0: \", np.count_nonzero( y_test == 0))\n","\n","print('Shape:', X_train.shape)\n","print('Type:', X_train.dtype)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"dRgt_rzpRQ7u","executionInfo":{"status":"ok","timestamp":1699436192129,"user_tz":-60,"elapsed":4,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["linear_classifier_accuracy=[]\n","MLP_classifier_accuracy=[]\n","CNN_classifier_accuracy=[]\n","DCNN_classifier_accuracy=[]\n","NN_model_sizes=[]"]},{"cell_type":"markdown","metadata":{"id":"w2ou5uc7qxMe"},"source":["# **NN models**\n","In this section we implement all general functions that can be used by all NN models"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"c9qUtpPD_W-e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699436217157,"user_tz":-60,"elapsed":20600,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}},"outputId":"48b5ebe8-4f17-4537-c634-f3c7b0e7709b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m10.2/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m61.4/128.9 kB\u001b[0m \u001b[31m714.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m989.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m989.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["pip install -q -U keras-tuner"]},{"cell_type":"markdown","source":["Import needed libraries"],"metadata":{"id":"s9lpEAQq8Nb1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HcTz4l1FV8a"},"outputs":[],"source":["!pip install scikit-optimize\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import SGDClassifier\n","import time\n","import tracemalloc\n","import sklearn\n","from keras.backend import binary_crossentropy\n","import tensorflow as tf\n","from tensorflow import keras\n","import keras_tuner as kt\n","from keras_tuner.tuners import BayesianOptimization\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"6i1nNvyeXnNq","executionInfo":{"status":"ok","timestamp":1699436547978,"user_tz":-60,"elapsed":224,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["epochs=150\n","batch_size=16\n","def NN_hyperparameter_fit(hyperparameter,Type_model):\n","  if (Type_model==\"CNN\"):\n","    model=build_model(hyperparameter)\n","  if (Type_model==\"DCNN\"):\n","    model=build_model_DCNN(hyperparameter)\n","  model.summary()\n","  history = model.fit(X_train, y_train, epochs=epochs,batch_size=batch_size, validation_data=(X_valid, y_valid))\n","  plot_model_performance(history,epochs)\n","  return model,history\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"pC_E7_A_qcs4","executionInfo":{"status":"ok","timestamp":1699436250186,"user_tz":-60,"elapsed":12,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def evaluate_NN_models(model_list):\n","  loss, accuracy , y_pred ,precision, recall ,f1_score ,support ,confusion_matrix_list = [], [], [], [], [], [], [], []\n","  for model in model_list:\n","    loss_value,accuracy_value=model.evaluate(X_test, y_test)\n","    loss.append(loss_value)\n","    accuracy.append(accuracy_value)\n","    y_pred_value=np.round(model.predict(X_test))\n","    y_pred.append(y_pred_value)\n","    precision_value, recall_value, f1_score_value, support_value = sklearn.metrics.precision_recall_fscore_support(y_test , y_pred_value, average= 'binary' )\n","    precision.append(precision_value)\n","    recall.append(recall_value)\n","    f1_score.append(f1_score_value)\n","    support.append(support_value)\n","    confusion_matrix_value=confusion_matrix(y_test, y_pred_value)\n","    confusion_matrix_list.append(confusion_matrix_value)\n","  return [accuracy ,precision, recall ,f1_score],confusion_matrix_list,y_pred\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"a9geD5BAKJUg","executionInfo":{"status":"ok","timestamp":1699436250186,"user_tz":-60,"elapsed":11,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def plot_model_performace_accuracy(history):\n","  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\n","  ax1.plot(history[0].history['accuracy'], 'g', label='Training Accuracy')\n","  ax1.plot(history[0].history['val_accuracy'], color='orange', label='Validation Accuracy')\n","  ax1.set_title('training and validation')\n","  ax1.set_xlabel('epoch')\n","  ax1.set_ylabel('accuracy')\n","  ax1.set_ylim(0.7, 1)\n","  ax1.grid(True)\n","  ax1.legend()\n","  ax2.plot(history[1].history['accuracy'], 'g', label='Training Accuracy')\n","  ax2.plot(history[1].history['val_accuracy'], color='orange', label='Validation Accuracy')\n","  ax2.set_title('training and validation')\n","  ax2.set_xlabel('epoch')\n","  ax2.set_ylabel('accuracy')\n","  ax2.set_ylim(0.7, 1)\n","  ax2.grid(True)\n","  ax2.legend()\n","  ax3.plot(history[2].history['accuracy'], 'g', label='Training Accuracy')\n","  ax3.plot(history[2].history['val_accuracy'], color='orange', label='Validation Accuracy')\n","  ax3.set_title('training and validation')\n","  ax3.set_xlabel('epoch')\n","  ax3.set_ylabel('accuracy')\n","  ax3.set_ylim(0.7, 1)\n","  plt.grid(True)\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"XTVrOjZnYxzq","executionInfo":{"status":"ok","timestamp":1699436250186,"user_tz":-60,"elapsed":11,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def plot_model_performance(history,number_of_epochs):\n","  print('CNN model training accuracy: ', history.history['accuracy'][number_of_epochs - 1])\n","  print('CNN model validation accuracy: ', history.history['val_accuracy'][number_of_epochs - 1])\n","  plt.figure()\n","  plt.plot(history.history['loss'], 'g', label='Training Loss')\n","  plt.plot(history.history['val_loss'], color='orange', label='Validation Loss')\n","  plt.title('Training and Validation Loss')\n","  plt.ylabel('Loss')\n","  plt.xlabel('epoch')\n","  plt.legend()\n","  plt.grid(True)\n","  plt.tight_layout()"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7-9-qnH9Q_nR","executionInfo":{"status":"ok","timestamp":1699436250700,"user_tz":-60,"elapsed":525,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","def plot_confusion_matrix(ax, conf_matrix, title, cmap):\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=cmap, cbar=False, ax=ax)\n","    ax.set_title(title)\n","\n","def draw_confusion_matrix(matrix,name):\n","  colormap=[\"Blues\",\"Greens\",\"Oranges\"]\n","  fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","  for i in range(len(matrix)):\n","    plot_confusion_matrix(axes[i], matrix[i], \"Confusion Matrix  \"+name+ str(i+1), colormap[i])\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"BF_BtmSWi9Ky","executionInfo":{"status":"ok","timestamp":1699436250700,"user_tz":-60,"elapsed":6,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["#### function for plotting tables\n","from tabulate import tabulate\n","def print_table(data,headers):\n","  table_data = list(zip(*data))\n","  table = tabulate(table_data, headers=headers, tablefmt='grid')\n","  print(table)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"82pFChydzcPy","executionInfo":{"status":"ok","timestamp":1699436250700,"user_tz":-60,"elapsed":5,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["#function to calculate the size of tradiotional machine learning models (e.g KNN, SVM, DT, etc..)\n","import joblib\n","import os\n","def calculate_ML_size(model,file_name):\n","  model_filename = \"/content/\"+file_name+\".joblib\"\n","  joblib.dump(model, model_filename)\n","  file_size = os.path.getsize(model_filename)\n","  return file_size"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"9ryKW2XHGO8B","executionInfo":{"status":"ok","timestamp":1699436250701,"user_tz":-60,"elapsed":6,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def ML_hyperparameters_study(classifier, param_grid, X_train_ML, X_test_ML, y_train_full_ML, y_test_full_ML,random=False):\n","    classifier.random_state = 42\n","    grid_search = GridSearchCV(classifier, param_grid, cv=5, verbose=1)\n","    grid_search.fit(X_train_ML, y_train_full_ML)\n","    results = grid_search.cv_results_\n","    mean_training_score_all = []\n","    params_all = []\n","    testing_accuracy = []\n","    models_size = []\n","    execution_time_list = []\n","    total_allocated_memory=[]\n","    average_allocated_memory=[]\n","    precision_list, recall_list, f1_score_list=[],[],[]\n","\n","    def fit_and_measure_memory(model, X_train_ML, y_train_full_ML):\n","      tracemalloc.start()\n","      start_time = time.time()\n","      model.fit(X_train_ML, y_train_full_ML)\n","      snapshot = tracemalloc.take_snapshot()\n","      tracemalloc.stop()\n","      end_time = time.time()\n","      execution_time_list.append(end_time - start_time)\n","      return snapshot\n","\n","    for mean_score, params in zip(results['mean_test_score'], results['params']):\n","        mean_training_score_all.append(mean_score)\n","        params_all.append(params)\n","        hyper_model = classifier\n","        if random:\n","            hyper_model.random_state = 42\n","        hyper_model.set_params(**params)\n","        snap = fit_and_measure_memory(hyper_model, X_train_ML, y_train_full_ML)\n","        # Calculate the total allocated memory from the snapshot\n","        total_memory=sum(stat.size for stat in snap.statistics('lineno'))\n","        total_allocated_memory.append(format_memory(total_memory))\n","        # Calculate the average allocated memory\n","        num_snapshots = len(snap.statistics('lineno'))\n","        average_allocated_memory.append(format_memory(total_memory / num_snapshots))\n","        y_pred = hyper_model.predict(X_test_ML)\n","        testing_accuracy.append(accuracy_score(y_test_full_ML, y_pred))\n","        precision_value, recall_value, f1_score_value, support_value = sklearn.metrics.precision_recall_fscore_support(y_test_full_ML , y_pred, average= 'weighted' )\n","        precision_list.append(precision_value)\n","        recall_list.append(recall_value)\n","        f1_score_list.append(f1_score_value)\n","        models_size.append(format_memory(calculate_ML_size(hyper_model, \"model(\" + str(mean_score) + \")\")))\n","\n","    best_params = grid_search.best_params_\n","    return [params_all,testing_accuracy,precision_list,recall_list,f1_score_list, models_size, execution_time_list,total_allocated_memory,average_allocated_memory], best_params ,mean_training_score_all"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"dLjH4-0ftyDD","executionInfo":{"status":"ok","timestamp":1699436250701,"user_tz":-60,"elapsed":5,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def convert_params_to_list(params_all):\n","  all_keys = set()\n","  for d in params_all:\n","      all_keys.update(d.keys())\n","\n","  data_values = [[d.get(key, None) for d in params_all] for key in all_keys]\n","  return data_values,all_keys"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"GwHHi40t55Z2","executionInfo":{"status":"ok","timestamp":1699436250701,"user_tz":-60,"elapsed":5,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def format_memory(memory_bytes):\n","    if memory_bytes < 1024:\n","        return f\"{memory_bytes} B\"\n","    elif memory_bytes < 1024 * 1024:\n","        return f\"{memory_bytes / 1024:.2f} KB\"\n","    elif memory_bytes < 1024 * 1024 * 1024:\n","        return f\"{memory_bytes / (1024 * 1024):.2f} MB\"\n","    else:\n","        return f\"{memory_bytes / (1024 * 1024 * 1024):.2f} GB\""]},{"cell_type":"code","source":["def calculate_model_sizes(model):\n","\n","  input_size = model.input_shape[1]\n","  output_size = model.output_shape[1]\n","  num_params = sum(tf.keras.backend.count_params(p) for p in model.trainable_variables)\n","  activation_units = 0\n","  for layer in model.layers:\n","      if isinstance(layer, tf.keras.layers.Dense):\n","          activation_units += layer.units\n","      elif isinstance(layer, tf.keras.layers.Conv2D):\n","          activation_units += layer.filters * (layer.input_shape[1] // layer.strides[0]) * (layer.input_shape[2] // layer.strides[1])\n","  return {\n","      \"input_size\": input_size,\n","      \"output_size\": output_size,\n","      \"num_params\": num_params,\n","      \"activation_units\": activation_units\n","  }"],"metadata":{"id":"ZWfN9Z08oKKU","executionInfo":{"status":"ok","timestamp":1699436250701,"user_tz":-60,"elapsed":4,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGsbnuckqxMc"},"source":["# **Linear Classifier (Baseline)**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3tmEM5ucRcMW"},"outputs":[],"source":["X_train_full_ML, X_test_ML, y_train_full_ML, y_test_full_ML = X[:train_size], X[train_size:], y[:train_size], y[train_size:]\n","X_train_ML= np.array([np.array(x_train).reshape(-1) for x_train in X_train_full_ML])\n","X_test_ML=np.array([np.array(x_test).reshape(-1) for x_test in X_test_ML])\n","\n","print('Shape:', X_train_ML.shape)\n","print('Type:', X_train_ML.dtype)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adHRRJNOqxMc"},"outputs":[],"source":["param_grid = {\n","    'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n","}\n","\n","models_info_linear,best_params_linear,mean_training_score_all_linear = ML_hyperparameters_study(SGDClassifier(),param_grid,X_train_ML, X_test_ML, y_train_full_ML, y_test_full_ML,random=True)\n","headers_linear=list(param_grid.keys())\n","\n","#### visulaize all hyperparameters with acuuracy and model size in one table\n","\n","headers_linear=['models','hyperparameters',' accuracy ','precision','recall','f1_score','model size','Runtime','total allocated memory','average allocated memory']\n","models_name=[]\n","for i in range(1, len(param_grid['alpha'])+1):\n","  models_name.append(\"Linear model \" + str(i))\n","data_linear=[models_name]\n","data_linear.extend(models_info_linear)\n","print_table(data_linear,headers_linear)\n","linear_classifier_accuracy=models_info_linear[1]\n"]},{"cell_type":"markdown","metadata":{"id":"ljcvgMjlivyr"},"source":["# **Fully connected NN (MLP)**"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"Ix-RR64vizZ7","executionInfo":{"status":"ok","timestamp":1699436682809,"user_tz":-60,"elapsed":239,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"outputs":[],"source":["def build_model_fc(number_of_hidden_layers,number_of_units,learning_rate=0.0001):\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Flatten())\n","    for i in range(number_of_hidden_layers):\n","      model.add(tf.keras.layers.Dense( number_of_units[i], activation='relu',kernel_initializer='glorot_uniform'))\n","      model.add(tf.keras.layers.Dropout(rate = 0.3))\n","    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","     # Compile the model with the hyperparameters\n","    model.compile(optimizer=optimizer,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'],)\n","    model.summary()\n","    history = model.fit(X_train, y_train, epochs=epochs,batch_size=batch_size, validation_data=(X_valid, y_valid))\n","    return model,history"]},{"cell_type":"markdown","source":["## Creating grid of hyperparameters values"],"metadata":{"id":"Zh8j8vb_Ojxv"}},{"cell_type":"code","source":["import random\n","\n","min_num_hidden_layers = 1  # Minimum number of hidden layers\n","max_num_hidden_layers = 5  # Maximum number of hidden layers\n","min_length = 32  # Minimum length of each hidden layer\n","max_length = 256  # Maximum length of each hidden layer\n","num_hidden_layers=[]\n","hidden_layer_lengths = []\n","num_layers = random.randint(min_num_hidden_layers, max_num_hidden_layers)\n","number_of_models=10\n","for _ in range(number_of_models):\n","    num_layers = random.randint(min_num_hidden_layers, max_num_hidden_layers)\n","    num_hidden_layers.append(num_layers)\n","    # Generate a list of random numbers between min_length and max_length and sort them in descending order\n","    layer_lengths = sorted([random.randint(min_length, max_length) for _ in range(num_layers)], reverse=True)\n","    hidden_layer_lengths.append(layer_lengths)\n","\n","\n","for i in range(len(num_hidden_layers)):\n","  print(f\"hidden layer :{num_hidden_layers[i]} , units: {hidden_layer_lengths[i]}\")"],"metadata":{"id":"qYi5zHzpOp-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QAyYZPImu9hv"},"source":["## In this section we will **build** and **train** Ten different MLP models each with different hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpLOsBcOqopo"},"outputs":[],"source":["models_list_MLP=[]\n","results_MLP=[]\n","for i in range(len(num_hidden_layers)):\n","  model,results=build_model_fc(num_hidden_layers[i],hidden_layer_lengths[i])\n","  models_list_MLP.append(model)\n","  results_MLP.append(results)\n","  plot_model_performance(results,epochs)"]},{"cell_type":"markdown","source":["## Choose the best three MLP models"],"metadata":{"id":"AJ29ZnS4O_eH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlxMOxyXq-Gs"},"outputs":[],"source":["result_all_models_FC,confusion_matrix_FC,predicitions_FC=evaluate_NN_models(models_list_MLP)\n","def find_top_three_indexes(lst):\n","    arr = np.array(lst)\n","    top_indexes = arr.argsort()[-3:][::-1]\n","    return top_indexes\n","\n","# Example usage:\n","top_three_indexes = find_top_three_indexes(result_all_models_FC[0])\n","print(\"Indexes of the top three values:\", top_three_indexes)"]},{"cell_type":"markdown","source":["## Let's train the three models with the entire training set:"],"metadata":{"id":"GccTxLr9PKkR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uROK3FZ_q-ou"},"outputs":[],"source":["MLP_best_models=[models_list_MLP[top_three_indexes[0]],models_list_MLP[top_three_indexes[1]],models_list_MLP[top_three_indexes[2]]]\n","model1=MLP_best_models[0]\n","model2=MLP_best_models[1]\n","model3=MLP_best_models[2]\n","results_total_train1 = model1.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train2 = model2.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train3 = model3.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)"]},{"cell_type":"code","source":["models_list_FC = []\n","models_list_FC.append(model1)\n","models_list_FC.append(model2)\n","models_list_FC.append(model3)"],"metadata":{"id":"kgW2e2BJPSdK","executionInfo":{"status":"ok","timestamp":1699436880436,"user_tz":-60,"elapsed":30,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnKNnhz8rElV"},"outputs":[],"source":["plot_model_performace_accuracy([results_MLP[top_three_indexes[0]],results_MLP[top_three_indexes[0]],results_MLP[top_three_indexes[0]]])"]},{"cell_type":"markdown","metadata":{"id":"QyXelBx3vL5u"},"source":["## Evaluate the best three MLP models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zkgO5oTss-Q"},"outputs":[],"source":["result_all_models_FC,confusion_matrix_FC,predicitions_FC=evaluate_NN_models(MLP_best_models)\n","\n","## create a table with all models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score']\n","\n","hyperparameters_values=[]\n","models=[]\n","for i in range(len(top_three_indexes)):\n","  hyperparameters_values.append(f\"{num_hidden_layers[top_three_indexes[i]]},{hidden_layer_lengths[top_three_indexes[i]]}\")\n","  models.append(f\"MLP Model{i}\")\n","data_normal_FC=[models]\n","data_normal_FC.append(hyperparameters_values)\n","data_normal_FC.extend(result_all_models_FC)\n","print_table(data_normal_FC,headers)\n","MLP_classifier_accuracy=result_all_models_FC[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81AM5PEcuou1"},"outputs":[],"source":["draw_confusion_matrix(confusion_matrix_FC,\"MLP model\")"]},{"cell_type":"markdown","metadata":{"id":"MHN6N68et-_P"},"source":["# **CNN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"knA-lZadHHEv"},"outputs":[],"source":["def build_model(hp,kernel_size=3):\n","\n","    # Tune the learning rate for the optimizer\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4,1e-5])\n","    # Tune the optimizer\n","    hp_optimizer = hp.Choice('optimizer', values=['adam'])\n","    # Tune the number of filters in the Conv2D layer\n","    hp_filters = hp.Int('filters', min_value=16, max_value=64, step=8)\n","    #tune the dropout values\n","    dropout_rate = hp.Choice('dropout', values=[0.3,0.5])\n","    #tuning weights initialization\n","    kernel_initializer = hp.Choice('kernel_initializer', values=['glorot_uniform', 'he_normal', 'lecun_normal'])\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Reshape(target_shape=(width, height, number_of_channels)))\n","    model.add(tf.keras.layers.Conv2D(filters = hp_filters , kernel_size=(kernel_size, kernel_size), activation='relu', kernel_initializer=kernel_initializer))\n","    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(32, activation='relu'))\n","    model.add(tf.keras.layers.Dense(1, kernel_initializer=kernel_initializer))\n","    model.add(tf.keras.layers.Activation('sigmoid'))\n","\n","     # Compile the model with the hyperparameters\n","    model.compile(optimizer=hp_optimizer,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'],)\n","    return model\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oyKybPUZG-RV"},"source":["## Let's observe the most performing models and identify their hyperparameters:"]},{"cell_type":"code","source":["# Let's use a Bayesian approach to conduct the search.\n","tuner = BayesianOptimization(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=20,\n","    #executions_per_trial=2,\n","    directory='tuner_single_dense',\n","    project_name='training_tuner_single_dense2'\n",")\n","\n","#Let's start training models with different hyperparameters.\n","tuner.search(X_train, y_train, epochs=75,batch_size=batch_size, validation_data=(X_valid, y_valid))"],"metadata":{"id":"0-zqjOFH-Alq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xxJO0iYoHi9U"},"outputs":[],"source":["num_trials = 15\n","best_hps2 = tuner.get_best_hyperparameters(num_trials=num_trials)\n","\n","for idx, hyperparameters in enumerate(best_hps2):\n","    print(f\"Set {idx + 1}: {hyperparameters.values}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aznstOiVV5M"},"outputs":[],"source":["best_hyperparameter_CNN=[best_hps2[3],best_hps2[1],best_hps2[2]]"]},{"cell_type":"markdown","metadata":{"id":"QL1DrH7wjPfC"},"source":["## Let's compare the accuracy trend on the training set and the validation set  for the 3 models with different hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdI_yVdlHsYu"},"outputs":[],"source":["model4,results4=NN_hyperparameter_fit(best_hyperparameter_CNN[0],\"CNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXnpHDyNJFLo"},"outputs":[],"source":["model5,results5=NN_hyperparameter_fit(best_hyperparameter_CNN[1],\"CNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29nxon6fJnUT"},"outputs":[],"source":["model6,results6=NN_hyperparameter_fit(best_hyperparameter_CNN[2],\"CNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V5VMP0aGr3QE"},"outputs":[],"source":["plot_model_performace_accuracy([results4,results5,results6])"]},{"cell_type":"markdown","metadata":{"id":"oaqmNKfujPfF"},"source":["## Let's train the three models with the entire training set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOyltvFRLzZy"},"outputs":[],"source":["results_total_train4 = model4.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train5 = model5.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train6 = model6.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WX0fxFp75eHl"},"outputs":[],"source":["models_list_CNN = []\n","models_list_CNN.append(model4)\n","models_list_CNN.append(model5)\n","models_list_CNN.append(model6)"]},{"cell_type":"markdown","metadata":{"id":"uF562MkZjPfG"},"source":["## Evaluate the CNN models on test set:\n","Evaluate All models and print out a table with all parameters (model , hyperparameter, accyracy, precision, recall, f1_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTipflbguwdp"},"outputs":[],"source":["result_all_models_CNN,confusion_matrix_CNN,predicitions=evaluate_NN_models(models_list_CNN)\n","## create a table with all models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score']\n","data_normal_CNN=[['CNN model 1',' CNN model 2', 'CNN model 3']]\n","hyperparameters_values=[]\n","for idx, hyperparameters in enumerate(best_hyperparameter_CNN):\n","  hyperparameters_values.append(hyperparameters.values)\n","data_normal_CNN.append(hyperparameters_values)\n","data_normal_CNN.extend(result_all_models_CNN)\n","print_table(data_normal_CNN,headers)\n","CNN_classifier_accuracy=result_all_models_CNN[0]\n"]},{"cell_type":"markdown","metadata":{"id":"Kkb3ECFBadPY"},"source":["Draw confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2laHmAfCafQm"},"outputs":[],"source":["draw_confusion_matrix(confusion_matrix_CNN,\"CNN model\")"]},{"cell_type":"markdown","metadata":{"id":"J4dgcL2Xd8Bx"},"source":["# **Deeper CNN (DCNN)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kl1zAgVepvHl"},"outputs":[],"source":["### number_of_convolutional : the number of convolutional layer\n","### kernal_size : a list of kernal sizes for each convolutional layer repespectively\n","def build_model_DCNN(hp,number_of_convolutional=2,kernal_size=[3,3]):\n","\n","    # Tune the learning rate for the optimizer\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2,1e-3,1e-4])\n","    # Tune the optimizer\n","    hp_optimizer = hp.Choice('optimizer', values=['adam'])\n","    # Tune the number of filters in the Conv2D layer\n","    hp_filters = hp.Int('filters', min_value=16, max_value=64, step=8)\n","    #tune the dropout values\n","    dropout_rate = hp.Choice('dropout', values=[0.3,0.5])\n","    #tuning weights initialization\n","    kernel_initializer = hp.Choice('kernel_initializer', values=['glorot_uniform', 'he_normal', 'lecun_normal'])\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Reshape(target_shape=(width, height, number_of_channels)))\n","\n","    for i in range(number_of_convolutional - 1):\n","      model.add(tf.keras.layers.Conv2D(filters = hp_filters , kernel_size=(kernal_size[i], kernal_size[i]), activation='relu',padding='same', kernel_initializer=kernel_initializer))\n","      model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","      model.add(tf.keras.layers.Dropout(rate = dropout_rate))\n","\n","    model.add(tf.keras.layers.Conv2D(filters = hp_filters , kernel_size=(kernal_size[len(kernal_size)-1], kernal_size[len(kernal_size)-1]), activation='relu',padding='same', kernel_initializer=kernel_initializer))\n","\n","\n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(32, activation='relu'))\n","    model.add(tf.keras.layers.Dense(1, kernel_initializer=kernel_initializer))\n","    model.add(tf.keras.layers.Activation('sigmoid'))\n","\n","    # Compile the model with the hyperparameters\n","    model.compile(optimizer=hp_optimizer,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n"]},{"cell_type":"markdown","source":["## Let's observe the most performing models and identify their hyperparameters:"],"metadata":{"id":"55wJBmnY6gMb"}},{"cell_type":"code","source":["# Let's use a Bayesian approach to conduct the search.\n","tuner = BayesianOptimization(\n","    build_model_DCNN,\n","    objective='val_accuracy',\n","    max_trials=5,\n","    #executions_per_trial=2,\n","    directory='tuner_single_dense',\n","    project_name='training_tuner_single_dense17'\n",")\n","\n","#Let's start training models with different hyperparameters.\n","tuner.search(X_train, y_train, epochs=75,batch_size=batch_size, validation_data=(X_valid, y_valid))"],"metadata":{"id":"_Dbj7qR2-d7P"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trssd2pSyOUT"},"outputs":[],"source":["num_trials = 6\n","best_hps_DCNN = tuner.get_best_hyperparameters(num_trials=num_trials)\n","for idx, hyperparameters in enumerate(best_hps_DCNN):\n","    print(f\"Set {idx + 1}: {hyperparameters.values}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gHg-f69aIcNX"},"outputs":[],"source":["best_hyperparameter_DCNN=[best_hps_DCNN[0],best_hps_DCNN[1],best_hps_DCNN[2]]"]},{"cell_type":"markdown","metadata":{"id":"JfPtfsRjyyTf"},"source":["## Train three models with the three best hyperparameters and study the performance of the model with validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rITqbBjgycvC"},"outputs":[],"source":["model7,results7=NN_hyperparameter_fit(best_hyperparameter_DCNN[0],\"DCNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpLrskL5yp1d"},"outputs":[],"source":["model8,results8=NN_hyperparameter_fit(best_hyperparameter_DCNN[1],\"DCNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2dFvtJMyp93"},"outputs":[],"source":["model9,results9=NN_hyperparameter_fit(best_hyperparameter_DCNN[2],\"DCNN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDGmglKZzCOq"},"outputs":[],"source":["plot_model_performace_accuracy([results7,results8,results9])"]},{"cell_type":"markdown","metadata":{"id":"tT2l5Hpcy9aD"},"source":["## Train the model on all the entire dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmXW-fQUzA0j"},"outputs":[],"source":["results_total_train7 = model7.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train8 = model8.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)\n","results_total_train9 = model9.fit(X_train_full, y_train_full, epochs=epochs,batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dlZ0S2HrzhPR"},"outputs":[],"source":["models_list_DCNN = []\n","models_list_DCNN.append(model7)\n","models_list_DCNN.append(model8)\n","models_list_DCNN.append(model9)"]},{"cell_type":"markdown","metadata":{"id":"6bXDmagDzvnu"},"source":["## Evaluate All models and print out a table with all parameters (model , hyperparameter, accyracy, precision, recall, f1_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bEieZUhFzwT0"},"outputs":[],"source":["result_all_models_DCNN,confusion_matrix_DCNN,predicitions=evaluate_NN_models(models_list_DCNN)\n","## create a table with all models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score']\n","data_normal_DCNN=[['DCNN model 1',' DCNN model 2', 'DCNN model 3']]\n","hyperparameters_values=[]\n","for idx, hyperparameters in enumerate(best_hyperparameter_DCNN):\n","  hyperparameters_values.append(hyperparameters.values)\n","data_normal_DCNN.append(hyperparameters_values)\n","data_normal_DCNN.extend(result_all_models_DCNN)\n","print_table(data_normal_DCNN,headers)\n","DCNN_classifier_accuracy=result_all_models_DCNN[0]"]},{"cell_type":"markdown","metadata":{"id":"33BXAYYZ0P0S"},"source":["Draw confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IMuVC7UJ0RuW"},"outputs":[],"source":["draw_confusion_matrix(confusion_matrix_DCNN,\"DCNN model\")"]},{"cell_type":"markdown","metadata":{"id":"UUU_a3XeqxMq"},"source":["\n","# **TensorFlow Lite model**\n"]},{"cell_type":"markdown","metadata":{"id":"fAK7VfJsbGfQ"},"source":["## Functions to evaluate and print analysis for  TFlite / quantized models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7SYQ3xYqxMr"},"outputs":[],"source":["import numpy as np\n","\n","def evaluate(model_file, X, y, categoricalAccuarcy):\n","    if(categoricalAccuarcy):\n","      accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","    else:\n","      accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","    interpreter = tf.lite.Interpreter(model_path = model_file)\n","    interpreter.allocate_tensors()\n","    input_details = interpreter.get_input_details()[0]\n","    output_details = interpreter.get_output_details()[0]\n","\n","    y_preds = []  # To store the predicted labels\n","    y_real = []\n","\n","    for x, y_true in zip(X,y):\n","        if input_details['dtype'] == np.uint8:\n","            input_scale, input_zero_point = input_details[\"quantization\"]\n","            x = x / input_scale + input_zero_point\n","        x = np.expand_dims(x, axis=0).astype(input_details[\"dtype\"])\n","        interpreter.set_tensor(input_details[\"index\"], x)\n","        interpreter.invoke()\n","        y_pred = interpreter.get_tensor(output_details[\"index\"])[0]\n","        accuracy.update_state(y_true, y_pred)\n","         # Collect the predicted labels\n","        if(categoricalAccuarcy):\n","            y_preds.append(np.argmax(y_pred))\n","        else:\n","            y_preds.append(np.round(y_pred[0]))\n","\n","        y_real.append(y_true)\n","    return accuracy.result(), y_preds, y_real"]},{"cell_type":"code","source":["def calculate_accuracy_quantization_full(y_prediction, y_real):\n","    if len(y_prediction) != len(y_real):\n","        raise ValueError(\"y_prediction and y_real must have the same length.\")\n","\n","    num_correct = sum(1 for pred, real in zip(y_prediction, y_real) if pred == real)\n","    total_predictions = len(y_prediction)\n","\n","    accuracy = num_correct / total_predictions\n","\n","    return accuracy"],"metadata":{"id":"X_CJYIyaQTyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AwkNvbTqxMr"},"outputs":[],"source":["\n","def evaluate_tflite_quantized_models(models_list,base_filename_model,models_files,full=False):\n","  tflite_model_accuracy,tflite_model_predicitons,y_true,precisions_tf_lite_model,recalls_tf_lite_model,f1_scores_tf_lite_model,confusion_tflite_matrix_list = [], [], [] ,[],[], [], []\n","  for i in range(len(models_list)):\n","    path = base_filename_model + '/' + models_files[i].name\n","    print(\"path\",path)\n","    accuracy, y_pred, y_real = evaluate(path, X_test, y_test, False)\n","    accuracy=accuracy.numpy()\n","    tflite_model_predicitons.append(y_pred)\n","    y_true.append(y_real)\n","    # print(f\"TFLite model {i+1} accuracy = {tflite_model_accuracy[i]:.4f}\")\n","    if full==True:\n","      y_pred = [np.round(value / 255) for value in y_pred]\n","      accuracy = calculate_accuracy_quantization_full(y_pred, y_real)\n","    tflite_model_accuracy.append(accuracy)\n","    precision_value, recall_value, f1_score_value, support_value = sklearn.metrics.precision_recall_fscore_support(y_real , y_pred, average= 'binary' )\n","    precisions_tf_lite_model.append(precision_value)\n","    recalls_tf_lite_model.append(recall_value)\n","    f1_scores_tf_lite_model.append(f1_score_value)\n","    confusion_matrix_value=confusion_matrix(y_test, y_pred)\n","    confusion_tflite_matrix_list.append(confusion_matrix_value)\n","  return [tflite_model_accuracy,precisions_tf_lite_model,recalls_tf_lite_model,f1_scores_tf_lite_model],confusion_tflite_matrix_list,tflite_model_predicitons\n"]},{"cell_type":"markdown","metadata":{"id":"1a-Z7foywbsu"},"source":["## Converting all models to TFlite model using tensorflow lite converter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVC6TsGTt5ku"},"outputs":[],"source":["# models_list=[model1,model2,model3,model4,model5,model6,model7,model8,model9]\n","#without DCNN models\n","models_list=[model1,model2,model3,model4,model5,model6]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7cNq4bhqxMq"},"outputs":[],"source":["tflite_models = []\n","for index in range(len(models_list)):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(models_list[index])\n","  tflite_models.append(converter.convert())"]},{"cell_type":"markdown","metadata":{"id":"VeAumtFjqxMq"},"source":["## It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data. We can store the models in a file in order to estimate its size:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hN1FkWBkqxMq"},"outputs":[],"source":["import pathlib\n","import os\n","tflite_models_dir = pathlib.Path(\"./\")\n","base_filename_model_tflite = 'tflite_model'\n","path = tflite_models_dir/base_filename_model_tflite\n","\n","if not os.path.exists(path):\n","  os.makedirs(path)\n","\n","tflite_model_files = []\n","tflite_model_size = []\n","\n","for i in range(1, len(models_list) +1):\n","  filename = f\"{base_filename_model_tflite}_{i}.tflite\"\n","  tflite_model_files.append(path/filename)\n","  tflite_model_files[i-1].write_bytes(tflite_models[i-1])\n","  tflite_model_size.append(os.path.getsize(tflite_model_files[i -1]) / float(2**10))\n","  print(\"TFlite model in KB:\", tflite_model_size[i-1])"]},{"cell_type":"markdown","metadata":{"id":"JF5adl6njPfJ"},"source":["## Evaluate and Calculate Accuracy, Precision, Recall, F1 Tf_lite_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mo3mhz-OZGfD"},"outputs":[],"source":["tflite_all_results, confusion_tflite_all, tflite_predictions = evaluate_tflite_quantized_models(models_list,base_filename_model_tflite,tflite_model_files)\n","## create a table with all tf lite models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score','model size KB']\n","data_tflite=[['tflite MLP model 1','tflite MLP model 2', 'tflite MLP model 3',\n","              'tflite CNN model 1','tflite CNN model 2', 'tflite CNN model 3',\n","              'tflite DCNN model 1','tflite DCNN model 2', 'tflite DCNN model 3']]\n","hyperparameters_values=[]\n","\n","for i in range(len(top_three_indexes)):\n","  hyperparameters_values.append(f\"{num_hidden_layers[top_three_indexes[i]]},{hidden_layer_lengths[top_three_indexes[i]]}\")\n","\n","\n","for idx, hyperparameters in enumerate(best_hyperparameter_CNN):\n","  hyperparameters_values.append(hyperparameters.values)\n","\n","# uncomment the following lines if the DCNN is used in the study\n","\n","# for idx, hyperparameters in enumerate(best_hyperparameter_DCNN):\n","#   hyperparameters_values.append(hyperparameters.values)\n","\n","data_tflite.append(hyperparameters_values)\n","data_tflite.extend(tflite_all_results)\n","data_tflite.append(tflite_model_size)\n","print_table(data_tflite,headers)\n","NN_model_sizes=tflite_model_size"]},{"cell_type":"markdown","metadata":{"id":"8JRZqEpUqxMr"},"source":["# **Post-training quantization**\n","\n","We can enable the default optimizations flag to quantize all fixed parameters (weights and biases) to 8-bit integers. Notice that scale and zero point for weights and bias can be calculated before the inference, becouse their ranges are already available. But how we can calculate scale and zero point for activations? We can use the **dynamic quantization**, in which scale and zero point for activations are calculated on-the-fly (online during inference). This means that the activations are always stored in float 32 and they are converted to integers while processing and back to floating point after the processing is done.\n"]},{"cell_type":"markdown","metadata":{"id":"xLvlcFezxB7z"},"source":["## TFlite Dynamic Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KvE7tJamqxMr"},"outputs":[],"source":["tflite_dynamic_quantized_models = []\n","\n","for i in range (0 , len(models_list)):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(models_list[i])\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  tflite_dynamic_quantized_models.append(converter.convert())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfbztuEiqxMs"},"outputs":[],"source":["tflite_dynamic_quantized_model_files = []\n","tflite_dynamic_quantized_model_size = []\n","\n","base_filename_dynamic = 'tflite_dynamic_quantized_model'\n","path = tflite_models_dir/base_filename_dynamic\n","\n","if not os.path.exists(path):\n","  os.makedirs(path)\n","\n","for i in range (1, len(models_list) + 1):\n","  file_name =  f\"{base_filename_dynamic}_{i}.tflite\"\n","  tflite_dynamic_quantized_model_files.append(path/file_name)\n","  tflite_dynamic_quantized_model_files[i-1].write_bytes(tflite_dynamic_quantized_models[i-1])\n","  tflite_dynamic_quantized_model_size.append(os.path.getsize(tflite_dynamic_quantized_model_files[i - 1]) / float(2**10))\n","  print(\"TFlite dynamic quantized model in KB:\", tflite_dynamic_quantized_model_size[i-1])"]},{"cell_type":"markdown","metadata":{"id":"uGcAO554criq"},"source":["### Evaluate and Calculate Accuracy, Precision, Recall, F1 Tf_lite_model_dynamic_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3JMRv6MXD46"},"outputs":[],"source":["dynamic_quantized_all_results, confusion_dynamic_quantized_all, dynamic_quantized_predictions = evaluate_tflite_quantized_models(models_list,base_filename_dynamic,tflite_dynamic_quantized_model_files)\n","## create a table with all tf lite models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score','model size KB']\n","data_dynamic_quantized=[['Dynamic quantized MLP model 1','Dynamic quantized MLP model 2', 'Dynamic quantized MLP model 3',\n","                         'Dynamic quantized CNN model 1','Dynamic quantized CNN model 2', 'Dynamic quantized CNN model 3',\n","                         'Dynamic quantized DCNN model 1','Dynamic quantized DCNN model 2', 'Dynamic quantized DCNN model 3']]\n","data_dynamic_quantized.append(hyperparameters_values)\n","data_dynamic_quantized.extend(dynamic_quantized_all_results)\n","data_dynamic_quantized.append(tflite_dynamic_quantized_model_size)\n","print_table(data_dynamic_quantized,headers)"]},{"cell_type":"markdown","metadata":{"id":"VnsT3qFDjPfP"},"source":["## Static Quantization\n","The model is now smaller with quantized weights with some decrease in the accuracy, but other variable data are still in float format. To quantize variable data (input/output and intermediates between layers), we can use **static quantization** to pre-computes scales and zero points also for all variable data in order to eliminate this overhead. However, we need some representative data in order to collect the distribution statistics for all the variable data and compute an estime of scales and zero points. The short-coming is that if the data is not representative, the scales and zero points computed might not reflect the true scenario during inference, and the inference accuracy will be harmed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnE_UlZkqxMt"},"outputs":[],"source":["def representative_data_gen():\n","  for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n","    yield [input_value]\n","\n","tflite_static_quantized_models = []\n","\n","for i in range (0 , len(models_list)):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(models_list[i])\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.representative_dataset = representative_data_gen\n","  tflite_static_quantized_models.append(converter.convert())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXfL2hewqxMt"},"outputs":[],"source":["tflite_static_quantized_model_files = []\n","tflite_static_quantized_model_size = []\n","\n","base_filename_static = 'tflite_static_quantized_model'\n","path = tflite_models_dir/base_filename_static\n","\n","if not os.path.exists(path):\n","  os.makedirs(path)\n","\n","for i in range (1, len(models_list) + 1):\n","  file_name =  f\"{base_filename_static}_{i}.tflite\"\n","  tflite_static_quantized_model_files.append(path/file_name)\n","  tflite_static_quantized_model_files[i - 1].write_bytes(tflite_static_quantized_models[i - 1])\n","  tflite_static_quantized_model_size.append(os.path.getsize(tflite_static_quantized_model_files[i - 1]) / float(2**10))\n","  print(\"TFlite static quantized model in KB:\", tflite_static_quantized_model_size[i-1])"]},{"cell_type":"markdown","metadata":{"id":"mx5wtz8xjPfQ"},"source":["### Evaluate and Calculate Accuracy, Precision, Recall, F1 Tf_lite_model_static_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ef3jL6hKqxMt"},"outputs":[],"source":["static_quantized_all_results, confusion_static_quantized_all, static_quantized_predictions = evaluate_tflite_quantized_models(models_list,base_filename_static,tflite_static_quantized_model_files)\n","## create a table with all tf lite models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score','model size KB']\n","data_static_quantized=[['Static quantized MLP model 1','Static quantized MLP model 2', 'Static quantized MLP model 3',\n","                        'Static quantized CNN model 1','Static quantized CNN model 2', 'Static quantized CNN model 3',\n","                        'Static quantized DCNN model 1','Static quantized DCNN model 2', 'Static quantized DCNN model 3']]\n","data_static_quantized.append(hyperparameters_values)\n","data_static_quantized.extend(static_quantized_all_results)\n","data_static_quantized.append(tflite_static_quantized_model_size)\n","print_table(data_static_quantized,headers)"]},{"cell_type":"markdown","metadata":{"id":"V3_niyOBqxMt"},"source":["## Full Static Quantization\n","Now all weights and variable data are quantized. However, to maintain compatibility with applications that traditionally use float model, the TensorFlow Lite Converter leaves the model input and output tensors in float. This is good for compatibility, but it won't be compatible with devices that perform only integer-based operations. To ensure end-to-end integer-only model, we need to specify some parameters to the converter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRfn9vIMqxMu"},"outputs":[],"source":["tflite_full_static_quantized_models = []\n","\n","for i in range(0, len(models_list)):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(models_list[i])\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.representative_dataset = representative_data_gen\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","  converter.inference_input_type = tf.uint8\n","  converter.inference_output_type = tf.uint8\n","  tflite_full_static_quantized_models.append(converter.convert())"]},{"cell_type":"markdown","metadata":{"id":"xFtAoEXSqxMu"},"source":["### The internal quantization remains the same as above, but we can see the input and output tensors are now integer format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iCvHPMMqxMu"},"outputs":[],"source":["interpreter = tf.lite.Interpreter(model_content = tflite_full_static_quantized_models[0])\n","input_type = interpreter.get_input_details()[0]['dtype']\n","print('input: ', input_type)\n","output_type = interpreter.get_output_details()[0]['dtype']\n","print('output: ', output_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLJ3UUXPqxMu"},"outputs":[],"source":["tflite_full_static_quantized_model_files = []\n","tflite_full_static_quantized_model_size = []\n","\n","base_filename_full_static = 'tflite_full_static_quantized_model'\n","path = tflite_models_dir/base_filename_full_static\n","\n","if not os.path.exists(path):\n","  os.makedirs(path)\n","\n","for i in range(1, len(models_list) + 1) :\n","  file_name =  f\"{base_filename_full_static}_{i}.tflite\"\n","  tflite_full_static_quantized_model_files.append(path/file_name)\n","  tflite_full_static_quantized_model_files[i - 1].write_bytes(tflite_full_static_quantized_models[i - 1])\n","  tflite_full_static_quantized_model_size.append(os.path.getsize(tflite_full_static_quantized_model_files[i - 1]) / float(2**10))\n","  print(\"TFlite full static quantized model in KB:\", tflite_full_static_quantized_model_size[i-1])"]},{"cell_type":"markdown","metadata":{"id":"uyYekcEYjPfT"},"source":["### Evaluate and Calculate Accuracy, Precision, Recall, F1 Tf_lite_model_full_static_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2KDJ1qtqxMu"},"outputs":[],"source":["full_static_quantized_all_results, confusion_full_static_quantized_all, full_static_quantized_predictions = evaluate_tflite_quantized_models(models_list,base_filename_full_static,tflite_full_static_quantized_model_files,full=True)\n","## create a table with all tf lite models results\n","headers=['models','hyperparameter','accuracy','precision','recall','f1_score','model size KB']\n","data_full_static_quantized=[['Full static quantized MLP model 1','full static quantized MLP model 2', 'full static quantized MLP model 3',\n","                             'Full static quantized CNN model 1','full static quantized CNN model 2', 'full static quantized CNN model 3',\n","                             'Full static quantized DCNN model 1','full static quantized DCNN model 2', 'full static quantized DCNN model 3']]\n","data_full_static_quantized.append(hyperparameters_values)\n","data_full_static_quantized.extend(full_static_quantized_all_results)\n","data_full_static_quantized.append(tflite_full_static_quantized_model_size)\n","print_table(data_full_static_quantized,headers)"]},{"cell_type":"markdown","metadata":{"id":"HmCqpGjTf-DT"},"source":["# **Best models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jla4e1q_2qv6"},"outputs":[],"source":["linear_index=linear_classifier_accuracy.index(max(linear_classifier_accuracy))\n","MLP_index=MLP_classifier_accuracy.index(max(MLP_classifier_accuracy))\n","CNN_index=CNN_classifier_accuracy.index(max(CNN_classifier_accuracy))\n","#un comment the following lines if the DCNN is used\n","\n","# DCNN_index=DCNN_classifier_accuracy.index(max(DCNN_classifier_accuracy))\n","# print(linear_index,MLP_index,CNN_index,DCNN_index)\n","\n","# tflite_modelfiles_best=[tflite_model_files[MLP_index],tflite_model_files[CNN_index + 3],tflite_model_files[DCNN_index + 6]]\n","tflite_modelfiles_best=[tflite_model_files[MLP_index],tflite_model_files[CNN_index + 3]]\n","# dynamic_modelfiles_best=[tflite_dynamic_quantized_model_files[MLP_index],tflite_dynamic_quantized_model_files[CNN_index + 3],tflite_dynamic_quantized_model_files[DCNN_index + 6]]\n","dynamic_modelfiles_best=[tflite_dynamic_quantized_model_files[MLP_index],tflite_dynamic_quantized_model_files[CNN_index + 3]]\n","# static_modelfiles_best=[tflite_static_quantized_model_files[MLP_index],tflite_static_quantized_model_files[CNN_index + 3],tflite_static_quantized_model_files[DCNN_index + 6]]\n","static_modelfiles_best=[tflite_static_quantized_model_files[MLP_index],tflite_static_quantized_model_files[CNN_index + 3]]\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aU0BlqEQjPfU"},"source":["# **Quantization Aware Training**\n","Quantization introduces information loss and therefore the inference accuracy from the quantized integer models are inevitably lower than that from the floating point models. Such information loss is due to that the floating points after quantization and de-quantization is not exactly recoverable. The idea of quantization aware training is to ask the neural network to take the effect of such information loss into account during training. We can use the TensorFlow Model Optimization toolkit and passing in input the Keras model. What that API is doing is extending that network with the ability to mimic the quantized behavior that would be happening during the inference time, during the training time.# Aware Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_80xQHD3rr8m"},"outputs":[],"source":["!pip install tensorflow-model-optimization"]},{"cell_type":"markdown","metadata":{"id":"cPAnK6Q0qKqx"},"source":["## Functions for building aware MLP, CNN and DCNN models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEDJvoWOqxMv"},"outputs":[],"source":["def build_model_aware_CNN(quantizator,kernel_size=3):\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Reshape(target_shape=(width, height, number_of_channels)))\n","    model.add(tf.keras.layers.Conv2D(24 , kernel_size=(kernel_size, kernel_size), activation='relu', kernel_initializer=\"glorot_uniform\"))\n","    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    model.add(tf.keras.layers.Dropout(rate = 0.3))\n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(32, activation='relu'))\n","    model.add(tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\"))\n","    model.add(tf.keras.layers.Activation('sigmoid'))\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","    quantization_aware_model = quantizator(model)\n","    # Compile the model with the hyperparameters\n","    quantization_aware_model.compile(optimizer=optimizer,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'],)\n","    quantization_aware_model.summary()\n","\n","    return quantization_aware_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7kxZ1mI9i9u"},"outputs":[],"source":["def build_model_aware_fc(quantizator,number_of_hidden_layers=1,number_of_units=128):\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Flatten())\n","    for i in range(1,number_of_hidden_layers+1):\n","      model.add(tf.keras.layers.Dense( number_of_units/i, activation='relu'))\n","      model.add(tf.keras.layers.Dropout(rate = 0.3))\n","    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n","\n","    quantization_aware_model = quantizator(model)\n","     # Compile the model with the hyperparameters\n","    quantization_aware_model.compile(optimizer=\"adam\",\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'],)\n","    quantization_aware_model.summary()\n","    return quantization_aware_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5VJoDOK6qdOG"},"outputs":[],"source":["def build_model_aware_DCNN(quantizator,number_of_convolutional=2,kernal_size=[5,3]):\n","\n","\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.InputLayer(input_shape=(width, height)))\n","    model.add(tf.keras.layers.Reshape(target_shape=(width, height, number_of_channels)))\n","\n","    for i in range(1,number_of_convolutional):\n","      model.add(tf.keras.layers.Conv2D(64 , kernel_size=(kernal_size[i], kernal_size[i]), activation='relu', kernel_initializer=\"glorot_uniform\"))\n","      model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","      model.add(tf.keras.layers.Dropout(rate = 0.3))\n","\n","    model.add(tf.keras.layers.Conv2D(64 , kernel_size=(kernal_size[len(kernal_size)-1], kernal_size[len(kernal_size)-1]), activation='relu', kernel_initializer=\"glorot_uniform\"))\n","\n","\n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(32, activation='relu'))\n","    model.add(tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\"))\n","    model.add(tf.keras.layers.Activation('sigmoid'))\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","    quantization_aware_model = quantizator(model)\n","    quantization_aware_model.compile(optimizer=optimizer,\n","                  loss='binary_crossentropy',\n","                  metrics=['accuracy'],)\n","    quantization_aware_model.summary()\n","    return quantization_aware_model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9592,"status":"ok","timestamp":1695371895521,"user":{"displayName":"ali dabbous","userId":"07190641217810416394"},"user_tz":-120},"id":"S_XJABxLzEaG","outputId":"020432ab-3d69-48ba-b5a4-bef29f69cb18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," quantize_layer (QuantizeLa  (None, 8, 8)              3         \n"," yer)                                                            \n","                                                                 \n"," quant_flatten (QuantizeWra  (None, 64)                1         \n"," pperV2)                                                         \n","                                                                 \n"," quant_dense (QuantizeWrapp  (None, 128)               8325      \n"," erV2)                                                           \n","                                                                 \n"," quant_dropout (QuantizeWra  (None, 128)               1         \n"," pperV2)                                                         \n","                                                                 \n"," quant_dense_1 (QuantizeWra  (None, 1)                 134       \n"," pperV2)                                                         \n","                                                                 \n","=================================================================\n","Total params: 8464 (33.06 KB)\n","Trainable params: 8449 (33.00 KB)\n","Non-trainable params: 15 (60.00 Byte)\n","_________________________________________________________________\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," quantize_layer_1 (Quantize  (None, 8, 8)              3         \n"," Layer)                                                          \n","                                                                 \n"," quant_reshape (QuantizeWra  (None, 8, 8, 1)           1         \n"," pperV2)                                                         \n","                                                                 \n"," quant_conv2d (QuantizeWrap  (None, 6, 6, 24)          291       \n"," perV2)                                                          \n","                                                                 \n"," quant_max_pooling2d (Quant  (None, 3, 3, 24)          1         \n"," izeWrapperV2)                                                   \n","                                                                 \n"," quant_dropout_1 (QuantizeW  (None, 3, 3, 24)          1         \n"," rapperV2)                                                       \n","                                                                 \n"," quant_flatten_1 (QuantizeW  (None, 216)               1         \n"," rapperV2)                                                       \n","                                                                 \n"," quant_dense_2 (QuantizeWra  (None, 32)                6949      \n"," pperV2)                                                         \n","                                                                 \n"," quant_dense_3 (QuantizeWra  (None, 1)                 38        \n"," pperV2)                                                         \n","                                                                 \n"," quant_activation (Quantize  (None, 1)                 1         \n"," WrapperV2)                                                      \n","                                                                 \n","=================================================================\n","Total params: 7286 (28.46 KB)\n","Trainable params: 7217 (28.19 KB)\n","Non-trainable params: 69 (276.00 Byte)\n","_________________________________________________________________\n"]}],"source":["## since our study on three different comination of the hyperparameters we will create three aware training quantizations models in order to compare the results\n","\n","import tensorflow_model_optimization as tfmot\n","quantizator = tfmot.quantization.keras.quantize_model\n","quantization_aware_model1=build_model_aware_fc(quantizator)\n","quantization_aware_model2=build_model_aware_CNN(quantizator)\n","quantization_aware_model3=build_model_aware_DCNN(quantizator)"]},{"cell_type":"markdown","metadata":{"id":"M6ccqFczqxMv"},"source":["## When we train the networks, it is implicitly learning to be resilient to the quantization error."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL-MNu4OqxMv"},"outputs":[],"source":["aware_results1 = quantization_aware_model1.fit(X_train, y_train, epochs=epochs,batch_size=batch_size, validation_data=(X_valid, y_valid))\n","aware_results2 = quantization_aware_model2.fit(X_train, y_train, epochs=epochs,batch_size=batch_size, validation_data=(X_valid, y_valid))\n","aware_results3 = quantization_aware_model3.fit(X_train, y_train, epochs=epochs,batch_size=batch_size, validation_data=(X_valid, y_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nu6zTcY_qxMw"},"outputs":[],"source":["plot_model_performace_accuracy([aware_results1,aware_results2,aware_results3])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRs0p9y1A32P"},"outputs":[],"source":["aware_model_list=[quantization_aware_model1,quantization_aware_model2,quantization_aware_model3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zcylnWd8qxMw"},"outputs":[],"source":["tflite_aware_quantized_models = []\n","for i in range(0, len(aware_model_list)):\n","  converter = tf.lite.TFLiteConverter.from_keras_model(aware_model_list[i])\n","  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","  converter.representative_dataset = representative_data_gen\n","  converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","  converter.inference_input_type = tf.uint8\n","  converter.inference_output_type = tf.uint8\n","  tflite_aware_quantized_models.append(converter.convert())\n","\n","interpreter = tf.lite.Interpreter(model_content = tflite_aware_quantized_models[0])\n","input_type = interpreter.get_input_details()[0]['dtype']\n","print('input: ', input_type)\n","output_type = interpreter.get_output_details()[0]['dtype']\n","print('output: ', output_type)\n","\n","tflite_aware_quantized_model_files = []\n","tflite_aware_quantized_model_size = []\n","\n","base_filename_aware = 'tflite_aware_quantized_model'\n","path = tflite_models_dir/base_filename_aware\n","\n","if not os.path.exists(path):\n","  os.makedirs(path)\n","\n","for i in range(1, len(aware_model_list) + 1) :\n","  file_name =  f\"{base_filename_aware}_{i}.tflite\"\n","  tflite_aware_quantized_model_files.append(path/file_name)\n","  tflite_aware_quantized_model_files[i - 1].write_bytes(tflite_aware_quantized_models[i - 1])\n","  tflite_aware_quantized_model_size.append(os.path.getsize(tflite_aware_quantized_model_files[i - 1]) / float(2**10))\n","  print(\"TFlite aware quantized model in KB:\", tflite_aware_quantized_model_size[i-1])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ouJIZDJRjPfW"},"source":["## Calculate Accuracy, Precision, Recall, F1 Tf_lite_aware_quantized_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygt18mgzqxMw"},"outputs":[],"source":["aware_quantized_all_results, confusion_aware_quantized_all, aware_quantized_predictions = evaluate_tflite_quantized_models(aware_model_list,base_filename_aware,tflite_aware_quantized_model_files,full=True)\n","## create a table with all tf lite models results\n","headers=['models''accuracy','precision','recall','f1_score','model size KB']\n","data_aware_quantized=[['Aware quantized MLP model ','Aware quantized model CNN ', 'Aware quantized DCNN model ']]\n","# units=\"\"\n","# for i in range(FC_best_hp[0]):\n","#   units + str(int(FC_best_hp[1]/i)) + \",\"\n","# hyperparameters_values=[\"{ hidden layers: 2 \" + str(FC_best_hp[0]) + \"units: 128, 64 \" + units +\" }\",CNN_best_hp.values,DCNN_best_hp.values]\n","# data_aware_quantized.append(hyperparameters_values)\n","data_aware_quantized.extend(aware_quantized_all_results)\n","data_aware_quantized.append(tflite_aware_quantized_model_size)\n","print_table(data_aware_quantized,headers)"]},{"cell_type":"markdown","metadata":{"id":"3ffK-qn1EZE3"},"source":["# **Benchmark Models with STM tools**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZrtTZTi01aq"},"outputs":[],"source":["# download from github requirement.txt\n","import requests\n","\n","github_file_url = 'https://github.com/STMicroelectronics/stm32ai-modelzoo/blob/main/requirements.txt'\n","local_file_path = 'requirements.txt'\n","\n","response = requests.get(github_file_url)\n","\n","if response.status_code == 200:\n","    json_data = response.json()  # Converti la risposta in JSON\n","    file_content = json_data['payload']['blob']['rawLines']  # Estrai il contenuto del file\n","\n","    with open(local_file_path, 'w') as file:\n","        for line in file_content:\n","            file.write(line + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQr7_l_YFXwE"},"outputs":[],"source":["#install package defined in requirement.txt\n","!pip install -r ./requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoSVfdhuEgGu"},"outputs":[],"source":["import os\n","os.environ[\"STM32AI_USERNAME\"] = \"ali.dabbous@edu.unige.it\"\n","os.environ[\"STM32AI_PASSWORD\"] = \"Moustafa2365!\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oB2Lz7IcKOfu"},"outputs":[],"source":["import sys\n","!{sys.executable} -m pip install pycurl seaborn numpy matplotlib\n","!{sys.executable} -m pip install ipywidgets\n","!{sys.executable} -m pip install gitdir\n","!{sys.executable} -m pip install shutils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kYxsji4L-No"},"outputs":[],"source":["import os\n","import shutil\n","# Get STM32Cube.AI Developer Cloud\n","!gitdir https://github.com/STMicroelectronics/stm32ai-modelzoo/tree/main/common/stm32ai_dc\n","\n","# Reorganize local folders\n","if os.path.exists('./stm32ai_dc'):\n","    shutil.rmtree('./stm32ai_dc')\n","shutil.move('./common/stm32ai_dc', './stm32ai_dc')\n","shutil.rmtree('./common')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B10qsQH1MJC2"},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import ipywidgets as widgets\n","\n","sys.path.append(os.path.abspath('stm32ai'))\n","os.environ['STATS_TYPE'] = 'jupyter_devcloud'\n","\n","os.makedirs('models', exist_ok=True)\n","os.makedirs('outputs', exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQpJKoM-paeN"},"outputs":[],"source":["import sys\n","import os\n","\n","# Append sys.path in order to add import folder for STM32AI\n","dir_name = os.path.dirname('./')\n","sys.path.insert(0, os.path.abspath(os.path.join(dir_name, '..')))\n","sys.path.append(os.path.abspath('../../../common'))\n","from stm32ai_dc import Stm32Ai, CloudBackend, CliParameters\n","from stm32ai_dc.errors import ParameterError, BenchmarkServerError\n","\n","# Get username/password from environment\n","username = os.environ.get('STM32AI_USERNAME', None)\n","password = os.environ.get('STM32AI_PASSWORD', None)\n","\n","results = []\n","\n","# Create STM32AI Class with Cloud Backend, given a username/password and a possible version\n","# Version set to \"None\" will use the latest version available in Developer Cloud\n","ai = Stm32Ai(CloudBackend(username, password, version=None))\n","\n","# List boards available for a benchmark in STM32Cube.AI Developer Cloud\n","boards = ai.get_benchmark_boards()\n","\n","\n","# Boards length should be greater than zero\n","# A length equals to zero mean a current maintenance or a failure\n","if len(boards) == 0:\n","    print(\"No board detected remotely, can't start benchmark\")\n","    sys.exit(0)\n","\n","boards\n"]},{"cell_type":"markdown","metadata":{"id":"1-X3Yp3epgXt"},"source":["# **Benchmark models on STM boards**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQ0WexL1aU0c"},"outputs":[],"source":["#uploading models to ST\n","def upload_model(number_of_models,base_file_name_model,model_file_names):\n","\n","  model_names=[]\n","  for i in range(number_of_models):\n","    path_model = base_file_name_model + '/' + model_file_names[i].name\n","    ai.upload_model(path_model)\n","    model_names.append(model_file_names[i].name)\n","  return model_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nd0yH0VxbFlr"},"outputs":[],"source":["#deleting model from ST\n","def delete_model(number_of_models,model_names):\n","  for i in range(number_of_models):\n","    model_name=model_names[i]\n","    ai.delete_model(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SXwGGe2bhtH"},"outputs":[],"source":["### Analyze and benchmark models\n","def analyze_benchmark_ST(board_name,model_name):\n","  activation_size,weights_size,macc,rom_size,ram_size,execution_time=[], [], [], [], [], []\n","  for i in range(len(model_name)):\n","    analyzing=ai.analyze(CliParameters(model=model_name[i]))\n","    print(analyzing)\n","    activation_size.append(analyzing.activations_size)\n","    weights_size.append(analyzing.weights)\n","    macc.append(analyzing.macc)\n","    rom_size.append(analyzing.rom_size)\n","    ram_size.append(analyzing.ram_size)\n","    benchamrking=ai.benchmark(CliParameters(model = model_name[i]), board_name)\n","    print(benchamrking)\n","    execution_time.append(benchamrking.graph['exec_time']['duration_ms'])\n","  return [activation_size,weights_size,macc,rom_size,ram_size,execution_time]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VQHmRuJ4j5a"},"outputs":[],"source":["##### benchmark and analyze models\n","def process_models_st(models_number,base_file_name,models_name,model_type):\n","  tflite_model_names=upload_model(models_number,base_file_name,models_name)\n","  data_tflite_board1,data_tflite_board2,data_tflite_board3,data_tflite_board4=[],[],[],[]\n","  print(\"models names\",tflite_model_names)\n","\n","  ######### benchmark and analyze dynamic quantized model with 'B_U585I_IOT02A' 4\n","  B_U585I_IOT02A_tflite_models_results=analyze_benchmark_ST('B-U585I-IOT02A',tflite_model_names)\n","  headers=[\"models\",\"activation_size\",\"weights_size\",\"macc\",\"rom_size\",\"ram_size\",\"execution_time\"]\n","  data_tflite_board1=[[model_type+\" MLP model \",model_type+\" CNN model \",model_type+\" DCNN model \"]]\n","  data_tflite_board1.extend(B_U585I_IOT02A_tflite_models_results)\n","\n","  ######### benchmark and analyze dynamic quantized model with 'STM32F469I-DISCO' 0\n","  STM32F469I_DISCO_tflite_models_results=analyze_benchmark_ST('STM32F469I-DISCO',tflite_model_names)\n","  data_tflite_board2=[[model_type+\" MLP model \",model_type+\" CNN model \",model_type+\" DCNN model \"]]\n","  data_tflite_board2.extend(STM32F469I_DISCO_tflite_models_results)\n","\n","  ######### benchmark and analyze dynamic quantized model with  'STM32L4R9I-DISCO' 1\n","  STM32L4R9I_DISCO_tflite_models_results=analyze_benchmark_ST('STM32L4R9I-DISCO',tflite_model_names)\n","  data_tflite_board3=[[model_type+\" MLP model \",model_type+\" CNN model \",model_type+\" DCNN model \"]]\n","  data_tflite_board3.extend(STM32L4R9I_DISCO_tflite_models_results)\n","\n","  ######### benchmark and analyze dynamic quantized model with  'STM32H7B3I-DK' 2\n","  STM32H7B3I_DK_tflite_models_results=analyze_benchmark_ST('STM32H7B3I-DK',tflite_model_names)\n","  data_tflite_board4=[[model_type+\" MLP model \",model_type+\" CNN model \",model_type+\" DCNN model \"]]\n","  data_tflite_board4.extend(STM32H7B3I_DK_tflite_models_results)\n","\n","  delete_model(models_number,tflite_model_names)\n","  print(\"B-U585I-IOT02A\")\n","  print_table(data_tflite_board1,headers)\n","  print(\"STM32F469I-DISCO\")\n","  print_table(data_tflite_board2,headers)\n","  print(\"STM32L4R9I-DISCO\")\n","  print_table(data_tflite_board3,headers)\n","  print(\"STM32H7B3I-D\")\n","  print_table(data_tflite_board4,headers)\n","  return B_U585I_IOT02A_tflite_models_results, STM32F469I_DISCO_tflite_models_results,STM32L4R9I_DISCO_tflite_models_results,STM32H7B3I_DK_tflite_models_results,data_tflite_board1,data_tflite_board2,data_tflite_board3,data_tflite_board4"]},{"cell_type":"markdown","source":["## Analyze and Benchmark best models on STM boards"],"metadata":{"id":"rppx4P8NQ9P_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9JSxSC0djKe"},"outputs":[],"source":["############# tflite model hardware analysis and benchmarking\n","B_U585I_IOT02A_tflite_models_result, STM32F469I_DISCO_tflite_models_result,STM32L4R9I_DISCO_tflite_models_result,STM32H7B3I_DK_tflite_models_result,data_tflite_model_board1,data_tflite_model_board2,data_tflite_model_board3,data_tflite_model_board4=process_models_st(len(tflite_modelfiles_best),base_filename_model_tflite,tflite_modelfiles_best,\"tflite\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KHmwqkAa6Y2N"},"outputs":[],"source":["############# dynamic quantized tflite model hardware analysis and benchmarking\n","B_U585I_IOT02A_dynamic_quantized_tflite_models_result, STM32F469I_DISCO_dynamic_quantized_tflite_models_result,STM32L4R9I_DISCO_dynamic_quantized_tflite_models_result,STM32H7B3I_DK_dynamic_quantized_tflite_models_result,data_dynamic_model_board1,data_dynamic_model_board2,data_dynamic_model_board3,data_dynamic_model_board4=process_models_st(len(dynamic_modelfiles_best),base_filename_dynamic,dynamic_modelfiles_best,\"dynamic quantized tflite\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf0Nzd2X6z6_"},"outputs":[],"source":["############# static quantized tflite model hardware analysis and benchmarking\n","B_U585I_IOT02A_static_quantized_tflite_models_result, STM32F469I_DISCO_static_quantized_tflite_models_result,STM32L4R9I_DISCO_static_quantized_tflite_models_result,STM32H7B3I_DK_static_quantized_tflite_models_result,data_static_model_board1,data_static_model_board2,data_static_model_board3,data_static_model_board4=process_models_st(len(static_modelfiles_best),base_filename_static,static_modelfiles_best,\"static quantized tflite\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBXkRi-SHSr9"},"outputs":[],"source":["############# aware quantized tflite model hardware analysis and benchmarking\n","B_U585I_IOT02A_aware_quantized_tflite_models_result, STM32F469I_DISCO_aware_quantized_tflite_models_result,STM32L4R9I_DISCO_aware_quantized_tflite_models_result,STM32H7B3I_DK_aware_quantized_tflite_models_result,data_aware_model_board1,data_aware_board2,data_aware_model_board3,data_aware_model_board4=process_models_st(len(tflite_aware_quantized_model_files),base_filename_aware,tflite_aware_quantized_model_files,\"aware quantized tflite\")\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["yK-ONXiYqxMW","FZvDBg5SqxMY","08rjB07ZqxMc","w2ou5uc7qxMe","iGsbnuckqxMc","ljcvgMjlivyr","Zh8j8vb_Ojxv","QAyYZPImu9hv","AJ29ZnS4O_eH","GccTxLr9PKkR","QyXelBx3vL5u","MHN6N68et-_P","oyKybPUZG-RV","QL1DrH7wjPfC","oaqmNKfujPfF","uF562MkZjPfG","J4dgcL2Xd8Bx","55wJBmnY6gMb","JfPtfsRjyyTf","tT2l5Hpcy9aD","6bXDmagDzvnu","UUU_a3XeqxMq","fAK7VfJsbGfQ","1a-Z7foywbsu","VeAumtFjqxMq","JF5adl6njPfJ","8JRZqEpUqxMr","xLvlcFezxB7z","VnsT3qFDjPfP","mx5wtz8xjPfQ","V3_niyOBqxMt","xFtAoEXSqxMu","HmCqpGjTf-DT","aU0BlqEQjPfU","cPAnK6Q0qKqx","M6ccqFczqxMv","ouJIZDJRjPfW","3ffK-qn1EZE3","1-X3Yp3epgXt","rppx4P8NQ9P_"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"nav_menu":{"height":"279px","width":"309px"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{},"toc_section_display":"block","toc_window_display":false}},"nbformat":4,"nbformat_minor":0}